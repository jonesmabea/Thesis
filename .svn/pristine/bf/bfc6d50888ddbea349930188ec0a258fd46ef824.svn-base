% !TeX spellcheck = <engl>
%
% File: chap01.tex
% Author: Victor F. Brena-Medina
% Description: Introduction chapter where the biology goes.
%
\let\textcircled=\pgftextcircled
\chapter{Introduction}
\label{chap:intro}

Accelerated by recent advancements in technology, the prospect of Autonomous Vehicles (AVs) driving in public roads is becoming more and more a reality. As this is an emerging field, there are numerous variations of implementations by different companies. Arguably, a key characteristic of these implementations is a large number of perception sensors including cameras, radars and Light detection ranging sensors(LiDAR) that are necessary for mapping the environment around the vehicle in order to safely navigate. Most of these sensors are quite expensive and fusing their input and processing it requires powerful processing units such as GPUs. This process consumes a lot of power and generates a lot of heat. In an effort to reduce the cost of such systems, companies are exploring different ways to reduce the number of sensors while still achieving a high level of navigational accuracy and safety. 

% used and in order for safe deployment of AVs there needs to be robust safety and testing procedures across all implementations. 
Compared to humans who are able to easily adapt to different situations, AVs need to be trained and tested in different possible situations. Different regions in the world pose varying challenges, such as poor visibility in Scandinavian countries due to heavy snow, and on the other hand heavily populated regions with heavy traffic such as New York pose challenges such as high occlusion of objects. An effective way of testing and evaluating AV systems is using a scenario-based approach. This has been the basis of Software-in-Loop and Hardware-in-Loop tests that test these systems in a virtual real-time environment to understand how they would react. This is done by modelling conditions and evaluating them on the corresponding software and hardware \cite{winkle2016development}. As a stepping stone, the scenario-based approach can be used to develop AV systems that are tailor-made for certain environments/regions and later on generalised as the systems become more adaptable to different environments. There are some companies and institutions that adopt this approach and have deployed autonomous prototypes in different areas. For example, Uber currently has some AVs in Steel City, Pittsburgh whereas Waymo has deployed some in Phoenix, Arizona. Other companies are still developing their AVs within test tracks\cite{miller_2018}. Using these approaches such companies are able to collect a large amount of data for testing and evaluating future models. However, this data is not always released to the public therefore limiting the variation and amount of data that is available to other parties working on AVs. 

Currently, a lot of cutting-edge research on AV systems relies on a small subset of data. Most research is based on the  KITTI dataset \cite{Geiger2012CVPR} that was collected to aid the development of AV systems by providing the necessary data to develop models to achieve different task. Such tasks included, stereo, optical, depth, odometry, scene flow and object detection. As a result it is highly favoured in the development of novel computer vision algorithm. 
In equal light, relying on few datasets creates a bottleneck in terms of validation as these algorithms may end up performing sub par when evaluated on different datasets or even worse in a real-time setting. 

Following this argument, a viable scenario that could be beneficial to explore is how different sensors and corresponding object detection models perform in urban and non-urban contexts. In doing so, companies developing AVs can fit different sensors in AVs that work in different contexts with the aim of maximising their performance in that context while reducing the cost by removing redundant sensors. The most reliable and accurate combination of sensors in terms of and reliability is camera and LiDAR. However, there has been increased interest in developing LiDAR only systems that are able to exploit highly accurate depth information from LiDAR such as Doxel\cite{doxel} that is an autonomous robot that uses LiDAR to monitor construction sites. As such it may be possible to develop LiDAR only units that can be used in a specific context. 

\section{Aims and Objectives}
Following the motivations in the presented discussion , the performance of single sensor ()LiDAR only) and multimodal(LiDAR and Camera) models in different contexts will be investigated with the aim of reducing the number of sensors in AVs.
To achieve this aim, the following objectives will need to be fulfilled:
\begin{enumerate}
	\item \textbf{Detect and characterise the context of images and point clouds.}
	%\item Evaluate the performance of sensors in different contexts. 
	\item \textbf{Evaluate the performance of single sensor and multimodal models in different contexts. }
	\item \textbf{Validate performance of the single sensor model on a custom dataset}
	%\item Legal, social and economic analysis of current implementations and proposed improvements. 
\end{enumerate}

\section{Deliverables}

The deliverables are categorized in terms of the related objectives: 
\begin{enumerate}
	\item \textbf{Context Detection}
	\begin{itemize}
		 \item \textbf{Image and LiDAR Context Classifier} - Available as a software deliverable in the form of Python scripts and Jupyter interactive notebooks. This will include pre-trained models to reproduce the results. 
	\end{itemize}
	\item \textbf{Custom single sensor and multimodal object detection models}
	\begin{itemize}
		\item \textbf{LiDAR only object detection model} modified to run on custom context datasets and external datasets. This will include tools to preprocess and convert external datasets. Available as Python scripts and Jupyter interactive notebooks.
		\item \textbf{LiDAR+Camera object detection model} modified to run on custom context datasets. This will include tools to preprocess these datasets. Available as Python scripts and Jupyter interactive notebooks.
		
	\end{itemize}
	\item \textbf{Single sensor validation}
	\begin{itemize}
		\item Point Cloud dataset obtained from the University of Bristol Smart Internet Lab working on connected AVs. Tools to convert the dataset into compatible input for LiDAR only object detection model and annotated sample frames for training and testing will be provided.
	\end{itemize}
	\item \textbf{Report containing the following discussions:}
	\begin{itemize}
		\item A review of related research and implementations tackling object detection in AVs. 
		\item Evaluation and analysis of context classifiers and object detection models in different contexts. 
		\item Validation results of single sensor model using external dataset from the Smart Internet Lab on connected and autonomous vehicles.
	\end{itemize}
\end{enumerate}

\section{Report structure}
The report will be split into different chapters each discussing various stages encountered while working on this project. 
\begin{itemize}

	\item Chapter 2 begins by discussing the background on the development of AVs, the different components in AVs, current implementations in the industry and finally related research on the research that has been undertaken in the field of object detection.
	
	\item Chapter 3 details the project execution by discussing the tools, methods and the criteria for various model evaluations.
	
	\item In Chapter 4, the results from various evaluations will be presented. This will be complemented with an analysis of the various evaluation metrics. 
	
	\item Finally, the concluding chapter will establish whether the objectives were achieved and a justification of their implication. Furthermore, a discussion on how this work can be further improved in future implementations will be presented. Lastly, a personal reflection on lessons learnt during the execution of the project will be detailed. 
\end{itemize}


