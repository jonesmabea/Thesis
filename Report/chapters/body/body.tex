% !TeX spellcheck = <engl>
\let\textcircled=\pgftextcircled


\chapter{Methodology}
\label{chap:methodology}


\section{Datasets}
	
While executing the project, three main datasets were used. 
	\begin{itemize}
		\item \textbf{KITTI }- KITTI is derived from a  VW station wagon that was driven in different traffic situations. The car was fitted with a number of sensors including a high resolution greyscale and colour cameras, Velodyne LiDAR and a GPS/IMU inertial navigation system. The dataset includes scenes from urban and rural contexts. 
		\item \textbf{CityScapes} -  Cityscapes contains multiple street scenes from 50 difference cities. In these scenes, 5000 are finely annotated at a pixel-level with 20000 coarsely annotated. 
	
		\item \textbf{University of Bristol Smart Internet Lab} - Derived from the connected and autonomous vehicles project, this dataset contains LiDAR capture files from a stationary vehicle positioned at the Millennium Square in Bristol. 
	\end{itemize}


\section{ Context Detection and Characterisation}

Determining the context is trivial in a mapped environment where you are able to infer from a map. However in a lab setting where you are only using image data from datasets this becomes quite difficult. This is mostly due to loss of temporal and spatial information if not prior specified or if so, done coarsely. Both VoxelNet and AVOD models were trained using the KITTI dataset using image frames, point clouds. The frames were discontigous and shuffled into training and testing directories. As such it was difficult to infer the context of the frames prior as no information was provided about the location they were captured in. Notably, this was also evident in the CityScapes dataset. From this observation, it was necessary to create a context detector for images and furthermore point clouds. 

In order to develop, test and train an image and point cloud context detector, I visually classified 3715 images from the KITTI dataset. The corresponding point clouds shared the same context as the classified images. 

\subsection*{Image Context Detection}
 Depending on the  number of cars, vegetation, road 
-Segmentation using Deeplab 
extracting labelled regions 
Used to extract regions of interest 


\subsection*{PointCloud Context Detection}


\title{Keypoint extractio }
Intrinsic Shape Signatures 

\title{Feature description}
Signature of Histogram of Orientations


\subsubsection*{Feature Matching}





Once the training dataset was classified into to the different contexts. I was able to train a classifier to determine the context of a pointcloud input. 





Conditional risk 


\subsubsection{Design}

\subsubsection{Evaluation}

Include pictures of results
	

\section{Context-Dependent Model Selection}
Introduction as to different modes of operation?
Multimodal vs Single model?
Modes of operation?

\subsubsection{VoxelNet }

\subsubsection{AVOD} 



\subsection{Conclusion}
Poor performance in terms of pedestrian and cyclists 
Good performance in non-urban areas 

\section{Validation with different datasets}


