{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pydriver\n",
    "import numpy as np \n",
    "import sklearn, sklearn.cluster, matplotlib.pyplot as plt\n",
    "import copy, datetime \n",
    "import glob, os\n",
    "import pandas as pd\n",
    "from sklearn import neighbors\n",
    "from sklearn.preprocessing import StandardScaler\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = \"/mnt/storage/home/ja17618/scratch/DATA_DIR/training/\"\n",
    "label_path ='/mnt/storage/home/ja17618/scratch/models/research/deeplab/datasets/labels.csv'\n",
    "f_lidar = glob.glob(os.path.join(path, 'velodyne', '*.bin'))\n",
    "f_rgb = glob.glob(os.path.join(path, 'image_2', '*.png'))\n",
    "f_labels = glob.glob(os.path.join(path, 'label_2', '*.txt'))\n",
    "data_tag = [name.split('/')[-1].split('.')[-2] for name in f_lidar]\n",
    "files=[tag+\".bin\" for tag in data_tag]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# point cloud coloring (False: reflectance, True: camera)\n",
    "USE_IMAGE_COLOR = False\n",
    "reader = pydriver.datasets.kitti.KITTIObjectsReader(path)\n",
    "reconstructor = pydriver.preprocessing.LidarReconstructor(\n",
    "    useImageColor=USE_IMAGE_COLOR,\n",
    "    removeInvisible=True,\n",
    "    )\n",
    "# mandatory ground truth categories\n",
    "DETECTION_CATEGORIES = ['car','van', 'truck', 'pedestrian', 'person_sitting', 'cyclist','tram']\n",
    "# optional ground truth categories\n",
    "DETECTION_CATEGORIES_OPT = ['misc', 'dontcare']\n",
    "\n",
    "MIN_OVERLAP = 0.7               # minimal overlap between 2D boxes\n",
    "EVALUATION_MODE = 'moderate'    # mode (easy, moderate, hard)\n",
    "VISUALIZE3D = False  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SHOT_RADIUS = 2.0\n",
    "preprocessor = pydriver.preprocessing.Preprocessor(reconstructor)\n",
    "keypointExtractor = pydriver.keypoints.ISSExtractor(salientRadius=0.25, nonMaxRadius=0.25)\n",
    "featureExtractor = pydriver.features.SHOTColorExtractor(shotRadius=SHOT_RADIUS, fixedY=-1.0)\n",
    "featureTypes = [('myfeature', featureExtractor.dims),]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function the vocabularies will use to create category storages\n",
    "def storageGenerator(dims, category):\n",
    "    sto = pydriver.detectors.vocabularies.Storage(dims, category,\n",
    "        preprocessors=[],\n",
    "        regressor=sklearn.neighbors.KNeighborsRegressor(n_neighbors=1),\n",
    "        )\n",
    "    return sto\n",
    "# function the detector will use to create vocabularies\n",
    "def vocabularyGenerator(dimensions, featureName):\n",
    "    voc = pydriver.detectors.vocabularies.Vocabulary(\n",
    "        dimensions,\n",
    "        preprocessors=[\n",
    "            sklearn.cluster.MiniBatchKMeans(n_clusters=100, batch_size=10, max_iter=100),\n",
    "            ],\n",
    "        classifier=sklearn.ensemble.AdaBoostClassifier(n_estimators=75),\n",
    "        storageGenerator=storageGenerator,\n",
    "        balanceNegatives=True,\n",
    "        )\n",
    "    return voc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize detector that will perform learning and recognition\n",
    "detector = pydriver.detectors.Detector(featureTypes, vocabularyGenerator=vocabularyGenerator)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "# context_df = pd.read_Ã·csv(\"labels.csv\",dtype={'frame': np.int32, 'context': np.str},index_col=0)\n",
    "# context_df['frame']=context_df['frame'].apply(lambda x: '{0:0>6}'.format(x))\n",
    "\n",
    "context_df = pd.read_csv(label_path,index_col=False,header=0)\n",
    "\n",
    "def get_context(frame):\n",
    "    c_df = context_df[context_df['frame']==frame]\n",
    "    return str(c_df['context'].values[0])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for image,pcl,label in zip(f_rgb,f_lidar,f_labels):\n",
    "#     print(\"File Number %s\" %(image.split('/')[-1].split('.')[-2]))\n",
    "#     raw_lidar = np.fromfile(pcl,dtype=np.float32).reshape((-1,4))\n",
    "#     scene = preprocessor.process(raw_lidar)\n",
    "#     input()\n",
    "np.set_printoptions(threshold=np.inf)\n",
    "timeStart = datetime.datetime.now()\n",
    "for frame in reader.getFramesInfo(1,10):\n",
    "    print('Training on frame %d...' % frame['frameId'])\n",
    "    scene = preprocessor.process(frame)\n",
    "    keypointCloud = keypointExtractor.getKeypointCloud(scene)\n",
    "    fkeypoints, features = featureExtractor.getFeatures(scene, keypointCloud)\n",
    "#     print(fkeypoints)\n",
    "    groundTruth, groundTruthOpt = pydriver.datasets.kitti.getKITTIGroundTruth(\n",
    "        frame['labels'],\n",
    "        DETECTION_CATEGORIES,\n",
    "        DETECTION_CATEGORIES_OPT,\n",
    "        mode='moderate',    # use moderate mode for training\n",
    "        )\n",
    "    category = get_context(frame['frameId'])\n",
    "    boxes3D_exclude = []\n",
    "    for label in groundTruth + groundTruthOpt:\n",
    "        box3D = pydriver.geometry.transform3DBox(label['box3D'], scene['transformation'])\n",
    "        # avoid training with non-negative examples\n",
    "        # the box is made bigger so SHOT features used as negatives\n",
    "        # don't capture parts of the object\n",
    "        box3D_exclude = copy.deepcopy(box3D)\n",
    "        box3D_exclude['dimensions']['height'] += 2*SHOT_RADIUS\n",
    "        box3D_exclude['dimensions']['width'] += 2*SHOT_RADIUS\n",
    "        box3D_exclude['dimensions']['length'] += 2*SHOT_RADIUS\n",
    "        boxes3D_exclude.append(box3D_exclude)\n",
    "        if category == 'urban':\n",
    "            if label in groundTruth:\n",
    "                # get keypoints which lie inside the labeled object box\n",
    "                boxKeypointCloud = keypointCloud.extractOrientedBoxes([box3D])\n",
    "                # extract features at these keypoints (and get new keypoints\n",
    "                # which depend on the feature extractor)\n",
    "                fkeypoints, features = featureExtractor.getFeatures(scene, boxKeypointCloud)\n",
    "                # learn new features and relations between features and objects\n",
    "                detector.addWords(category, 'myfeature', features, fkeypoints, box3D)\n",
    "\n",
    "        \n",
    "        # get keypoints which lie outside of labeled object boxes\n",
    "    negativeKeypointCloud = keypointCloud.extractOrientedBoxes(boxes3D_exclude, invert=True)\n",
    "    # extract features at these keypoints\n",
    "    fkeypoints, features = featureExtractor.getFeatures(scene, negativeKeypointCloud)\n",
    "    # learn features associated with absence of objects\n",
    "    detector.addWords('negative', 'myfeature', features)\n",
    "            \n",
    "timeTraining = datetime.datetime.now() - timeStart \n",
    "# perform learning on stored data\n",
    "print('Learning...')\n",
    "timeStart = datetime.datetime.now()\n",
    "detector.learn(nStorageMaxRandomSamples=25000)\n",
    "timeLearning = datetime.datetime.now() - timeStart\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluator = pydriver.evaluation.Evaluator(minOverlap=MIN_OVERLAP, nPoints=100)\n",
    "# perform testing with frames which were not used for training\n",
    "firstFrame = 10\n",
    "lastFrame = 10 + 10\n",
    "timeStart = datetime.datetime.now()\n",
    "for frame in reader.getFramesInfo(firstFrame, lastFrame):\n",
    "    print('Testing on frame %d...' % frame['frameId'])\n",
    "    # see the training part above\n",
    "    scene = preprocessor.process(frame)\n",
    "    keypointCloud = keypointExtractor.getKeypointCloud(scene)\n",
    "    groundTruth, groundTruthOpt = pydriver.datasets.kitti.getKITTIGroundTruth(\n",
    "        frame['labels'],\n",
    "        DETECTION_CATEGORIES,\n",
    "        DETECTION_CATEGORIES_OPT,\n",
    "        mode=EVALUATION_MODE,\n",
    "        )\n",
    "\n",
    "    # extract keypoints and features for the whole scene\n",
    "    fkeypoints, features = featureExtractor.getFeatures(scene, keypointCloud)\n",
    "    # perform recognition on extracted features\n",
    "    detections = detector.recognize({'myfeature': (fkeypoints,features)})\n",
    "\n",
    "    # convert 3D detections (NumPy array) to labels (list of\n",
    "    # dictionaries) that include 2D box projections used for evaluation\n",
    "    # and revert the transformation of the scene, so they have the same\n",
    "    # coordinate system as the original KITTI labels\n",
    "    detections_labels = pydriver.datasets.detections2labels(\n",
    "        detections,\n",
    "        scene['transformation'].I,    # inverse matrix\n",
    "        frame['calibration']['projection_left'],\n",
    "        scene['img_left'].shape,\n",
    "        )\n",
    "    # exclude detections which are always considered optional by\n",
    "    # KITTI (i.e. in 'hard' mode) and will not positively contribute\n",
    "    # to performance\n",
    "    detections_labels = [l for l in detections_labels if \\\n",
    "                          l['info']['truncated'] <= 0.5 and \\\n",
    "                          l['box2D']['bottom']-l['box2D']['top'] >= 25.0\n",
    "                        ]\n",
    "\n",
    "    # add frame recognition results to evaluator\n",
    "    evaluator.addFrame(groundTruth, groundTruthOpt, detections_labels)\n",
    "    if VISUALIZE3D:\n",
    "        # perform visualization in the transformed cloud\n",
    "        # convert ground truth labels to ground truth detections\n",
    "        gtd = pydriver.datasets.labels2detections(groundTruth, scene['transformation'])\n",
    "        gtdOpt = pydriver.datasets.labels2detections(groundTruthOpt, scene['transformation'])\n",
    "        scene['cloud'].visualizeDetections(detections, gtd, gtdOpt)\n",
    "timeEvaluation = datetime.datetime.now() - timeStart\n",
    "\n",
    "# show evaluation results\n",
    "print(\"Training time: %s\" % timeTraining)\n",
    "print(\"Learning time: %s\" % timeLearning)\n",
    "print(\"Evaluation time: %s\" % timeEvaluation)\n",
    "print(\"Average precision: %.2f\" % evaluator.aprecision)\n",
    "print(\"Average orientation similarity: %.2f\" % evaluator.aos)\n",
    "values = evaluator.getValues()\n",
    "plt.figure()\n",
    "plt.plot(values['recall'], values['precision'],\n",
    "    label='Precision (AP %0.2f)' % evaluator.aprecision)\n",
    "plt.plot(values['recall'], values['OS'],\n",
    "    label='Orientation similarity (AOS %0.2f)' % evaluator.aos)\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.ylim([0.0, 1.05])\n",
    "plt.xlabel('Recall')\n",
    "plt.ylabel('Precision / OS')\n",
    "plt.legend(loc=\"upper right\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Found input variables with inconsistent numbers of samples: [3, 3712]",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-39-5c9698a3fb59>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0mY\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mget_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mid\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mid\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdata_tag\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0mclf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mneighbors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mKNeighborsRegressor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m \u001b[0mclf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mY\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;31m# for pcl in f_lidar[:20]:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/vox/lib/python3.6/site-packages/sklearn/neighbors/base.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y)\u001b[0m\n\u001b[1;32m    743\u001b[0m         \"\"\"\n\u001b[1;32m    744\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mKDTree\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mBallTree\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 745\u001b[0;31m             \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcheck_X_y\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"csr\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmulti_output\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    746\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_y\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    747\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/vox/lib/python3.6/site-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36mcheck_X_y\u001b[0;34m(X, y, accept_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, multi_output, ensure_min_samples, ensure_min_features, y_numeric, warn_on_dtype, estimator)\u001b[0m\n\u001b[1;32m    581\u001b[0m         \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat64\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    582\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 583\u001b[0;31m     \u001b[0mcheck_consistent_length\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    584\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    585\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/vox/lib/python3.6/site-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36mcheck_consistent_length\u001b[0;34m(*arrays)\u001b[0m\n\u001b[1;32m    202\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0muniques\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    203\u001b[0m         raise ValueError(\"Found input variables with inconsistent numbers of\"\n\u001b[0;32m--> 204\u001b[0;31m                          \" samples: %r\" % [int(l) for l in lengths])\n\u001b[0m\u001b[1;32m    205\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    206\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Found input variables with inconsistent numbers of samples: [3, 3712]"
     ]
    }
   ],
   "source": [
    "context_df = pd.read_csv(label_path,index_col=False,header=0)\n",
    "context_df['frame']=context_df['frame'].apply(lambda x: '{0:0>6}'.format(x))\n",
    "def get_context(id):\n",
    "    f = context_df[context_df['frame']==id]\n",
    "    f = f['context'].values[0]\n",
    "    return f\n",
    "x = context_df.loc[:,'frame'].values\n",
    "y = context_df.loc[:,['context']].values\n",
    "\n",
    "X = np.array([np.fromfile(pcl,dtype=np.float32).reshape() for pcl in f_lidar[:10]])\n",
    "data_tag = [name.split('/')[-1].split('.')[-2] for name in f_lidar]\n",
    "Y = [get_context(id) for id in data_tag]\n",
    "clf = neighbors.KNeighborsRegressor()\n",
    "clf.fit(X,Y)\n",
    "\n",
    "# for pcl in f_lidar[:20]:\n",
    "#     id = pcl.split('/')[-1].split('.')[-2]\n",
    "#     x = np.fromfile(pcl,dtype=np.float32).reshape((-1,4))\n",
    "#     y = get_context(id)\n",
    "#     clf.fit(x,y)\n",
    "\n",
    "\n",
    "cmap_light = ListedColormap(['#FFAAAA', '#AAFFAA', '#AAAAFF'])\n",
    "cmap_bold = ListedColormap(['#FF0000', '#00FF00', '#0000FF'])\n",
    "\n",
    "# Plot the decision boundary. For that, we will assign a color to each\n",
    "# point in the mesh [x_min, x_max]x[y_min, y_max].\n",
    "x_min, x_max = X[:, 0].min() - 1, X[:, 0].max() + 1\n",
    "y_min, y_max = X[:, 1].min() - 1, X[:, 1].max() + 1\n",
    "xx, yy = np.meshgrid(np.arange(x_min, x_max, h),\n",
    "                     np.arange(y_min, y_max, h))\n",
    "Z = clf.predict(np.c_[xx.ravel(), yy.ravel()])\n",
    "\n",
    "# Put the result into a color plot\n",
    "Z = Z.reshape(xx.shape)\n",
    "plt.figure()\n",
    "plt.pcolormesh(xx, yy, Z, cmap=cmap_light)\n",
    "\n",
    "# Plot also the training points\n",
    "plt.scatter(X[:, 0], X[:, 1], c=y, cmap=cmap_bold,\n",
    "            edgecolor='k', s=20)\n",
    "plt.xlim(xx.min(), xx.max())\n",
    "plt.ylim(yy.min(), yy.max())\n",
    "plt.title(\"3-Class classification (k = %i, weights = '%s')\"\n",
    "          % (n_neighbors, weights))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:vox]",
   "language": "python",
   "name": "conda-env-vox-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
